## [Sams, Manninen, Surakka, Helin, Kättö, 1998] 
Sams, M., Manninen, P., Surakka, V., Helin, P., & Kättö, R. (1998). McGurk effect in Finnish syllables, isolated words, and words in sentences: Effects of word meaning and sentence context. Speech Communication, 26(1), 75-87. 

**Subjects**: 65 undergraduate students (62 females, 19-45 years old), most were volunteers 

**Stimuli**: One female talker in front of a white background with her head supported against the wall to avoid movements. She uttered each stimulus several times and the most neutral and clear tokens were selected for the experiment. Informants saw the talker-face on a computer screen as the distance of 60 cm. They were told that the stimuli might be meaningful and meaningless but not about McGurk effect. Subjects had 6 s to answer before the next stimuli started.  

**1.	/pa/ and /ka/**

*	Combines and expected to give rise in most cases to /ta/ or /ka/ perceptions 
*	Acoustical /ka/ was dubbed on to the visual presentation of /ka/ (often perceived as /pka/ or /kpa/) 

**2.	19 different solitary shot natural words, containing phonemes /p/ and /k/ in various vowel context** 

*	Words like /pannu/ were combined with visual perception of  /kannu/ -> expected /tannu/ (non-word) or /kannu/ (meaningful) 
*	Combinations: 
    -	Meaningful (audio) + meaningful (video) -> expected: non-word/video 
    -	Meaningless (audio) + meaningless (video) 

**3.	42 different short natural Finnish words and non-words**, containing phonemes /p/ and /k/ in various vowel context, **presented in different location of three-words sentences.** (the length up to 6 s) **WHITE NOISE OF 53 dB!!!!!** 

*	Combinations: 
   -	Meaningful (audio) + meaningless (video) -> non-word/video 
   -	Meaningless (audio) + meaningless (video) 
   -	Discordant word could be the first, the middle and the last 

**Results (first words and non-word of the sentence):** 

acoustically meaningful sentences: 

* 17% audio 
*	26% video 
*	25% something else 
*	**13% fusion non-word** 
*	19% other non-word 
 auditory and visual non-words: 
*	11% audio 
*	43% video 
*	15% something else 
*	**29% fusion non-word** 
*	2% other non-word 

Differed statictically significantly  
  
**Results (last words and non-word of the sentence):** 

acoustically meaningful sentences: 

* 8% audio 
*	33% video 
*	9% something else 
*	**35% fusion non-word** 
*	15% other non-word 
 auditory and visual non-words: 
*	8% audio  
*	27% video 
*	18% something else 
*	**45% fusion non-word** 
*	2% other non-word 


**Omissions:** (простите, что конспект здесь непонятный, но я просто переписывала, что они дают в статье) 

* occurred in /u/ (2/3) - initial consonants were not perceived in 50% 
*	words started acoustically with /pa/ (10/16) - 7% 
*	/p/ in /i/ context (2/3) - 2.3% 
*	occurred in /e/ with /p/ 
  
 The McGurk effect is very robust - in the experiment it modifies the perception approximately in 90% of the subjects.  
 
## [Dekle, Fowler, Funnell 1992]  

Dekle, D. J., Fowler, C. A., & Funnell, M. G. (1992). Audiovisual integration in perception of real words. Attention, Perception, & Psychophysics, 51(4), 355-362. 

*Trying to explain:* 

+ the motor theory (e.g., Liberman & Mattingly, 1985) 
+ the direct-realist theory (Fowler, 1986; Rosenblum, 1989) 
(in both: listeners to speech are claimed to perceive linguistically significant actions of the vocal tract, which are signaled to different degrees both optically and acoustically.) 
+ the fuzzy-logical theory of speech perception (Massaro, 1987) 
(any cue associated with production of a syllable in experience, and therefore in memory, can serve as information for the syllable. 

> Experiment 1.  

**Subjects**: 33 undergraduated students who participated for course credit in an introductory psychology
course 

**Stimuli**: Five randomized instances of each of nine trial types (types are listed in Table 1)  

![Soundtrack and Video Word Pairs Used in Experiment 1](https://github.com/astafyevai/mcgurk2017-2018/blob/master/dekle_1.jpg)

(The stimuli dubbing was accomplished by filtering one token of each of the nine auditory words at 10 kHz, digitizing them at a sampling rate of 20 kHz) 

*Results* 

- no-view condition - 97% auditory selections, 3% McGurk responses 
- a view condition - 17% audio selections, 79% McGurk responses, 4% video responses 

> Experiment 2. 

**Subjects**: 36 undergraduates from the same population as those who participated in Experiment 1 

**Stimuli**: The same as in Experiment 1. 

*Results* (Both groups were asked 10 circle the word they [saw] the model say)  

- no-audio condition - 69% video selections, 31% McGurk responses 
- with-audio condition - 55% audio selections, 38% McGurk responses, 7% video responses 

[Their conclusion:] McGurk effects occur on nonwords, they occur on words as well; under those same conditions, influences on lipreading (reverse McGurk) are weaker than McGurk effects. 

> Experiment 3.

The McGurk effect itself gives rise to a marked change in the perceiver's phenomenal experience of hearing a word. It seemed possible that the strong McGurk effect and the weak lipreading effect might have different origins. The task was to compare a sequential pair of mouthed words and to decide whether they were the same or different words (lipreading condition).  

**Subjects**: 24 undergraduates from the same population as those who participated in previous experiments/ 12 - hearing condition (McGurk effect), other 12 - the lipreading condition (reverse McGurk effect).

**Stimulus materials**: Five sets of three dubbed word pairs in the lipreading condition and another five sets in the McGurk condition (subjects were instructed to ignore information in the other sensory modality). Stimuli vs. Not-stimuli = 50/50  

*Results*  
- the subjects' tendency to judge a same pair of words as visibly the same word (lipreading test) or as audibly the same word (hearing test) differently  
- on the lipreading (reverse McGurk) test - 36% of trials were judged differently  
- on the hearing test 81% of trials were judged them as the same word repeated (the difference between the conditions is significant)  
- those in Easton and Basala (1982) were the same in lipreading and McGurk conditions but were biased toward **a strong lipreading effect**  

## [Easton, Basala 1982]  
(это у которых ничего не получилось. почему? авторы предыдущей законспектированной работы считают, что 
1. Неправильно подобранные стимулы, которые не особенно различаются внешне;
2. Эффект чтения по губам, а это != эффекту МакГурка;
3. **МакГурк = фонетика (не лексика)**!)

EASTON, R. D., &: BASALA, M. (1982). Perceptual dominance during lipreading. Perception & Psychophysics, 32, 562-570.  
NB!:  The manner-place hypothesis put forth by MacDonald and McGurk seems to account for much of the data (but see Summerfield, 1987, for some qualifications).  

"The perceptual task was performed in the presenceof substantial white noise (sufficient to reduce auditory recognition to 50%60%)"
 
> Experiment 1. Standart leapreading indification test (SLT), Dubbed leapreading indification test (DLT), Standart leapreading multiple choice test (SLT), Dubbed leapreading multiple choice test (DLT)

**Subjects**  46 randomly selected students (20 subjects in the first group, 20 subjects in the second group, 6 subjects in control group):
- group 1 - SLT 
        * Criteria for SLT:
                1. CORRECT- the word that was lipped
                2. INITIAL having same initial visual information as that of lipped word
                3. FINAL having same final visual information as that of lipped word
                4. BOTH having same initial and final visual information as lipped word
                5. NEITHER in which all visual information differed from that of the lipped word.
- group 2 - DLT
        * Criteria for DLT:
                1. SAME-dubbed word same as lipped word
                2. INITIAL-dub having same initial visual information as that of lipped word
                3. FINAL-dub having same final visual information as that of lipped word
                4. BOTH-dub having same initial and final visual information as lipped word
                5. NEITHER-dub in which all visual information differed from that of the lipped word.  
                
![Examples for DLT](https://github.com/astafyevai/mcgurk2017-2018/blob/master/dekle_1.jpg)  
                
**Stimuli:**  
* 30 words in one test: 15 monosyllabic and 15 spondaic
* For each subject - two different tests (indification anf multiple choice)

*Results:*  
- SLR was superior to DLR
- multiple choice performance was superior to identification 
- spondaic words resulted in greater accuracy of lipreading than monosyllabic words
- observers can read single words on the lips
- discrepant information is simultaneously present lipreading accuracy drops to about 5% (under visual-auditory discrepancy conditions -  improved to about 42%)
- errors were distributed equally across the four different dubbing categories (for both types of words)
- auditory error comprised 32% of the total error
- monosyllabic words resulted in more auditory error than spondaics
- auditory error was smallest when the initial and final visualand auditoryphonemesdidnot correspond (NEITHER category)

"It was found that the accuracy of perceived visual speech (lips) was substantially reduced by the presence of discrepant auditory information."  
"Monosyllabic words were more susceptible to the auditory biasing than were compound words, and the greater the visual-auditory discrepancy (visible-consonant), the smaller the auditory bias effect."

> Experiment 2. 

(with small amounts of discordance, visual bias is substantial, but with larger amounts of discordance, visual bias is found to be lessened (Warren & Cleaves, 1971;Warren, Welch, & McCarthy, 1981))

**Subjects:** 60 undergraduates were assigned randomly to the three groups.

**Stimuli**:  In each of the three tests, the same 32 monosyllabic words were used as the visual stimuli
1. a standard lipreading (SLR) test in which monosyllabic words were presented to observers, who then were required to identify the lipped word
2. a high compelling test - dubbed lipreading (DLR), a woman speaker was videotaped as the visual stimulus and her voice was dubbed over as the auditory stimulus
3. a low compelling test - dubbed lipreading (DLR), a man's voice was used to dub the auditory information over the videotape of the speakingwoman  

stimuli vs. fillers = 1/2
dubbing category criteria: BOTH and NEITHER  

**asked subjects to rate how confident they were that their response was correct on a five-point scale (1 = complete guess, 3 = modera~y confident,. S = absolutely confident).**  

*Results:*  
for the SLR  
* the mean cofidence associated with correct responses was 3.0
* associated with incorrect responses was 2.3
* the difference between these average confidence values was significant

for the DLR  

* the low- and highcompellingness subjects did not differ in lipreading accuracy
* confidence in correct responses was 3.7
* associated with incorrect responses was 2.3
* revealed a significant interaction between the compellingness and dub category variables
* accuracy for the low-compellingness group under the NEITHER dub category resulted in significantly greater accuracy than did the other three conditions
* large visual-auditory discordance results in the least disruption of lipreading performance
* The use of auditory dubs to create a visual-auditory speech discrepancydemonstrated that auditory information does indeed dominate during speech recognition
